{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden van de dataset\n",
    "dataset = pd.read_csv('./CasusData.csv')  # Vervang 'jouw_bestandsnaam.csv' door de werkelijke bestandsnaam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset na filtering en imputatie:\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "1   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "2   100.0   100.0   100.0   100.0   100.0   100.0   100.0   -97.0   100.0   \n",
      "3   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "4   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0   100.0  ...   100.0 -7541.2643  4.864921e+06    2.0         1.0    106.0   \n",
      "1   100.0  ...   100.0 -7536.6212  4.864934e+06    2.0         1.0    106.0   \n",
      "2   100.0  ...   100.0 -7519.1524  4.864950e+06    2.0         1.0    103.0   \n",
      "3   100.0  ...   100.0 -7524.5704  4.864934e+06    2.0         1.0    102.0   \n",
      "4   100.0  ...   100.0 -7632.1436  4.864982e+06    0.0         0.0    122.0   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID     TIMESTAMP  \n",
      "0               2.0     2.0     23.0  1.371714e+09  \n",
      "1               2.0     2.0     23.0  1.371714e+09  \n",
      "2               2.0     2.0     23.0  1.371714e+09  \n",
      "3               2.0     2.0     23.0  1.371714e+09  \n",
      "4               2.0    11.0     13.0  1.369910e+09  \n",
      "\n",
      "[5 rows x 529 columns]\n"
     ]
    }
   ],
   "source": [
    "# Behandeling van ontbrekende waarden\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset_imputed = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "print(\"Dataset na filtering en imputatie:\")\n",
    "print(dataset_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (normalisatie)\n",
    "# Hier gebruiken we StandardScaler om de features te normaliseren.\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(dataset_imputed.iloc[:, :520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset na clustering:\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "1   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "2   100.0   100.0   100.0   100.0   100.0   100.0   100.0   -97.0   100.0   \n",
      "3   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "4   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   100.0   \n",
      "\n",
      "   WAP010  ...  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0   100.0  ... -7541.2643  4.864921e+06    2.0         1.0    106.0   \n",
      "1   100.0  ... -7536.6212  4.864934e+06    2.0         1.0    106.0   \n",
      "2   100.0  ... -7519.1524  4.864950e+06    2.0         1.0    103.0   \n",
      "3   100.0  ... -7524.5704  4.864934e+06    2.0         1.0    102.0   \n",
      "4   100.0  ... -7632.1436  4.864982e+06    0.0         0.0    122.0   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID     TIMESTAMP  LOCATION_CLUSTER  \n",
      "0               2.0     2.0     23.0  1.371714e+09                 2  \n",
      "1               2.0     2.0     23.0  1.371714e+09                 2  \n",
      "2               2.0     2.0     23.0  1.371714e+09                 2  \n",
      "3               2.0     2.0     23.0  1.371714e+09                 2  \n",
      "4               2.0    11.0     13.0  1.369910e+09                 2  \n",
      "\n",
      "[5 rows x 530 columns]\n"
     ]
    }
   ],
   "source": [
    "# K-means clustering voor groepering van locaties\n",
    "features_scaled = scaler.fit_transform(dataset_imputed.iloc[:, :520])\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "dataset_imputed['LOCATION_CLUSTER'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "print(\"Dataset na clustering:\")\n",
    "print(dataset_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nieuwe kolom: WALKING_PATTERN (bijvoorbeeld op basis van locatieverandering)\n",
    "dataset_imputed['WALKING_PATTERN'] = ((dataset_imputed['LATITUDE'].diff() != 0) | (dataset_imputed['LONGITUDE'].diff() != 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007',\n",
      "       'WAP008', 'WAP009', 'WAP010',\n",
      "       ...\n",
      "       'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION',\n",
      "       'USERID', 'PHONEID', 'TIMESTAMP', 'LOCATION_CLUSTER',\n",
      "       'WALKING_PATTERN'],\n",
      "      dtype='object', length=531)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_imputed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groeperen op USERID:\n",
      "   USERID  WALKING_DIRECTION  LOCATION_CLUSTER\n",
      "0     1.0                  1                 2\n",
      "1     2.0                  1                 2\n",
      "2     3.0                  1                 0\n",
      "3     4.0                  1                 2\n",
      "4     5.0                  1                 2\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering voor wandelrichting op basis van patronen\n",
    "dataset_imputed['WALKING_DIRECTION'] = dataset_imputed['LATITUDE'].diff().apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Groeperen op individuele gebruikers\n",
    "grouped_data = dataset_imputed.groupby('USERID').agg({\n",
    "    'WALKING_DIRECTION': 'max',  # Aggregeer de wandelrichting (max waarde over tijd)\n",
    "    'LOCATION_CLUSTER': 'max',  # Aggregeer de locatiecluster (max waarde over tijd)\n",
    "    # Voeg andere gewenste aggregaties toe voor extra informatie\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Groeperen op USERID:\")\n",
    "print(grouped_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aangepaste functie om de modus te berekenen voor een SeriesGroupBy\n",
    "def mode_groupby(series_groupby):\n",
    "    # Gebruik scipy.stats.mode om de modus te berekenen\n",
    "    modes = mode(series_groupby)[0]\n",
    "    # Neem de eerste modus (als er meerdere zijn)\n",
    "    return modes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007',\n",
      "       'WAP008', 'WAP009', 'WAP010',\n",
      "       ...\n",
      "       'FLOOR', 'BUILDINGID', 'SPACEID', 'RELATIVEPOSITION', 'USERID',\n",
      "       'PHONEID', 'TIMESTAMP', 'LOCATION_CLUSTER', 'WALKING_PATTERN',\n",
      "       'WALKING_DIRECTION'],\n",
      "      dtype='object', length=532)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_imputed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groeperen op individuele gebruikers (opnieuw)\n",
    "grouped_data = dataset_imputed.groupby('USERID').agg({\n",
    "    'WALKING_PATTERN': mode_groupby,  # Gebruik aangepaste functie voor modus\n",
    "    'LOCATION_CLUSTER': 'max'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwijder eventuele NaN-waarden die zijn ontstaan na groeperen\n",
    "grouped_data = grouped_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features en doelvariabele\n",
    "features = grouped_data[['LOCATION_CLUSTER']]\n",
    "target = grouped_data['WALKING_PATTERN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de data opnieuw in trainings- en testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_imputed['WALKING_PATTERN'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doelvariabele\n",
    "target = dataset_imputed['WALKING_PATTERN']\n",
    "\n",
    "# Features voor voorspelling (bijvoorbeeld locatiecluster)\n",
    "features = dataset_imputed[['LOCATION_CLUSTER']]\n",
    "\n",
    "# Split de data in trainings- en testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train een Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorspel de WALKING_PATTERN op de testset\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by USERID\n",
    "grouped_data = dataset_imputed.groupby('USERID').agg({\n",
    "    'WALKING_PATTERN': 'max',\n",
    "    'LOCATION_CLUSTER': 'max',\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "target = grouped_data['WALKING_PATTERN']\n",
    "features_grouped = grouped_data[['LOCATION_CLUSTER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.unique(y_train)\n",
    "print(unique_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# You can try 'lbfgs', 'newton-cg', etc.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model and make predictions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\K1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1528\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1523\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1527\u001b[0m     )\n\u001b[1;32m-> 1528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([n_iter_])\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\K1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1143\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes_) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1143\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m             \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1148\u001b[0m         )\n\u001b[0;32m   1150\u001b[0m     class_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(class_weight, classes\u001b[38;5;241m=\u001b[39mclasses_, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "# Instantiate the logistic regression model with a different solver\n",
    "classifier = LogisticRegression(solver='liblinear')  # You can try 'lbfgs', 'newton-cg', etc.\n",
    "\n",
    "# Fit the model and make predictions\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train LogisticRegression\u001b[39;00m\n\u001b[0;32m     15\u001b[0m lr_classifier \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mlr_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m lr_predictions \u001b[38;5;241m=\u001b[39m lr_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train Neural Network\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\K1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1554\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1552\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1559\u001b[0m     )\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1562\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_grouped, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Train DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "\n",
    "# Train LogisticRegression\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "lr_predictions = lr_classifier.predict(X_test)\n",
    "\n",
    "# Train Neural Network\n",
    "nn_classifier = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_classifier.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "nn_predictions = (nn_classifier.predict(X_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
